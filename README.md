VideoLanguageFuturePrediction
===
VLEP dataset for video and language future event prediction. 

\[What is More Likely to Happen Next? Video-and-Language Future Event Prediction\]

[Jie Lei](http://www.cs.unc.edu/~jielei/), [Licheng Yu](http://www.cs.unc.edu/~licheng/),
[Tamara L. Berg](http://tamaraberg.com/), [Mohit Bansal](http://www.cs.unc.edu/~mbansal/)


Table of Contents
=================

  * [VideoLanguageFuturePrediction](#videolanguagefutureprediction)
  * [Dataset](#vlep-dataset)
  * [Evaluation and CodaLab Submission](#evaluation-and-codalab-submission)
  * [Related work](#related-work)
  * [Citation](#citation)
  * [Contact](#contact)


## VLEP Dataset
The dataset is released at [data](data), please see [data/README.md](data/README.md) for details. 


## Evaluation and CodaLab Submission

We only release ground-truth answers for train and dev splits. To get results on the test split, 
please submit your results to our CodaLab evaluation server following the instructions here:
[standalone_eval/README.md](standalone_eval/README.md).



## Related Work
- [TVC (Video+Dialogue Captioning)](https://github.com/jayleicn/TVCaption) 
- [TVR (Video+Dialogue Retrieval)](https://github.com/jayleicn/TVRetrieval) 
- [TVQA (Localized Video QA)](https://github.com/jayleicn/TVQA)
- [TVQA+ (Spatio-Temporal Video QA)](https://github.com/jayleicn/TVQAplus)
- [recurrent-transformer (coherent video paragraph captioning)](https://github.com/jayleicn/recurrent-transformer)


## Citation
If you find this code useful for your research, please cite our paper:
```
@inproceedings{lei2020vlep,
  title={What is More Likely to Happen Next? Video-and-Language Future Event Prediction},
  author={Lei, Jie and Yu, Licheng and Berg, Tamara L and Bansal, Mohit},
  booktitle={EMNLP},
  year={2020}
}
```

## Contact
Jie Lei, jielei@cs.unc.edu
